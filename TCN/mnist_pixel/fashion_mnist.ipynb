{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/home/A00512318/TCN')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from TCN.mnist_pixel.model import TCN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "def data_generator(root, batch_size):\n",
    "    train_set = datasets.FashionMNIST(root=root, train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "    test_set = datasets.FashionMNIST(root=root, train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTCN(ep):\n",
    "    global steps\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda: \n",
    "            data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, input_channels, seq_length)\n",
    "        if permutee:\n",
    "            data = data[:, :, permute]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        steps += seq_length\n",
    "        if batch_idx > 0 and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tSteps: {}'.format(\n",
    "                ep, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), train_loss/log_interval, steps))\n",
    "            train_losses_[ep-1].append(train_loss/log_interval)\n",
    "            train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTCN():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    correct_class = np.zeros([10])\n",
    "    correct_total = np.zeros([10])\n",
    "    confusion_matrix = np.zeros([10, 10])\n",
    "#     print(confusion_matrix)\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            if cuda:\n",
    "                data, target = data.to(device), target.to(device) # move devices to cuda\n",
    "            data = data.view(-1, input_channels, seq_length)\n",
    "            if permutee:\n",
    "                data = data[:, :, permute]\n",
    "#             data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "            c = (pred == target).squeeze()\n",
    "#             c = (predicted == labels).squeeze()\n",
    "#         for i in range(4):\n",
    "#             label = labels[i]\n",
    "#             class_correct[label] += c[i].item()\n",
    "#             class_total[label] += 1\n",
    "            size_check = c.size()[0]\n",
    "#             print(size_check)\n",
    "            for i in range(size_check):\n",
    "#                 print(pred[i].item(), target.data.view_as(pred)[i].item())\n",
    "#                 label = pred[i].item()\n",
    "                label = target[i]\n",
    "                correct_class[label] += c[i].item()\n",
    "                correct_total[label] += 1\n",
    "#                 print(label)\n",
    "                t = pred[i].item()\n",
    "                confusion_matrix[label][t] += 1\n",
    "#                 print(i)\n",
    "#                 if (label == t):\n",
    "#                     print(c[i].item())\n",
    "#                     correct_class[label] += c[i].item()\n",
    "#                 correct_total[label] += 1\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracies_.append(correct.item() / 10000.)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    print(confusion_matrix)\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * correct_class[i] / correct_total[i]))\n",
    "    return test_loss, confusion_matrix\n",
    "\n",
    "# testTCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37010"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save(model, file_name):\n",
    "    torch.save(model, file_name)\n",
    "    print('Saved as %s' % save_filename)\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open('{0}.pkl'.format(name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('{0}.pkl'.format(name), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "model = TCN(input_channels, n_classes, hidden_units=nhid, levels=levels, \n",
    "            kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters size => 37010\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.298547\tSteps: 79184\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.701038\tSteps: 157584\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.594310\tSteps: 235984\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.527355\tSteps: 314384\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.481741\tSteps: 392784\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.458747\tSteps: 471184\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.431339\tSteps: 549584\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.432346\tSteps: 627984\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.418199\tSteps: 706384\n",
      "\n",
      "Test set: Average loss: 0.4465, Accuracy: 8389/10000 (83%)\n",
      "\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.396668\tSteps: 814576\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.393481\tSteps: 892976\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.395305\tSteps: 971376\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.376072\tSteps: 1049776\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.377098\tSteps: 1128176\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.373792\tSteps: 1206576\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.359076\tSteps: 1284976\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.366681\tSteps: 1363376\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.362760\tSteps: 1441776\n",
      "\n",
      "Test set: Average loss: 0.4031, Accuracy: 8530/10000 (85%)\n",
      "\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.351733\tSteps: 1549968\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.347430\tSteps: 1628368\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.356231\tSteps: 1706768\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.335847\tSteps: 1785168\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.341032\tSteps: 1863568\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.338655\tSteps: 1941968\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.326596\tSteps: 2020368\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.333838\tSteps: 2098768\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.333584\tSteps: 2177168\n",
      "\n",
      "Test set: Average loss: 0.3743, Accuracy: 8640/10000 (86%)\n",
      "\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.321369\tSteps: 2285360\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.316582\tSteps: 2363760\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.325546\tSteps: 2442160\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.314324\tSteps: 2520560\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.321528\tSteps: 2598960\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.313587\tSteps: 2677360\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.306921\tSteps: 2755760\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.313238\tSteps: 2834160\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.316102\tSteps: 2912560\n",
      "\n",
      "Test set: Average loss: 0.3543, Accuracy: 8674/10000 (86%)\n",
      "\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.300391\tSteps: 3020752\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.294092\tSteps: 3099152\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.313004\tSteps: 3177552\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.297043\tSteps: 3255952\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.304171\tSteps: 3334352\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.297523\tSteps: 3412752\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.291195\tSteps: 3491152\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.295946\tSteps: 3569552\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.299162\tSteps: 3647952\n",
      "\n",
      "Test set: Average loss: 0.3387, Accuracy: 8754/10000 (87%)\n",
      "\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.286815\tSteps: 3756144\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.280126\tSteps: 3834544\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.299880\tSteps: 3912944\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.281313\tSteps: 3991344\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.287403\tSteps: 4069744\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.283092\tSteps: 4148144\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.278093\tSteps: 4226544\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.282422\tSteps: 4304944\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.287897\tSteps: 4383344\n",
      "\n",
      "Test set: Average loss: 0.3444, Accuracy: 8763/10000 (87%)\n",
      "\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.276475\tSteps: 4491536\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.266573\tSteps: 4569936\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.291463\tSteps: 4648336\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.265891\tSteps: 4726736\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.276418\tSteps: 4805136\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.272979\tSteps: 4883536\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.267739\tSteps: 4961936\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.272317\tSteps: 5040336\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.276728\tSteps: 5118736\n",
      "\n",
      "Test set: Average loss: 0.3254, Accuracy: 8818/10000 (88%)\n",
      "\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.264116\tSteps: 5226928\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.255219\tSteps: 5305328\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.281627\tSteps: 5383728\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.259440\tSteps: 5462128\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.263539\tSteps: 5540528\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.262373\tSteps: 5618928\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.257349\tSteps: 5697328\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.260798\tSteps: 5775728\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.268733\tSteps: 5854128\n",
      "\n",
      "Test set: Average loss: 0.3227, Accuracy: 8803/10000 (88%)\n",
      "\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.254356\tSteps: 5962320\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.246298\tSteps: 6040720\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.275482\tSteps: 6119120\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.250479\tSteps: 6197520\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.254760\tSteps: 6275920\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.256038\tSteps: 6354320\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.244760\tSteps: 6432720\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.253706\tSteps: 6511120\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.258367\tSteps: 6589520\n",
      "\n",
      "Test set: Average loss: 0.3251, Accuracy: 8800/10000 (88%)\n",
      "\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.245563\tSteps: 6697712\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.236571\tSteps: 6776112\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.265306\tSteps: 6854512\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.242601\tSteps: 6932912\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.242938\tSteps: 7011312\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.249254\tSteps: 7089712\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.235857\tSteps: 7168112\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.244895\tSteps: 7246512\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.249756\tSteps: 7324912\n",
      "\n",
      "Test set: Average loss: 0.3163, Accuracy: 8833/10000 (88%)\n",
      "\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.231745\tSteps: 7433104\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.215178\tSteps: 7511504\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.232870\tSteps: 7589904\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.211360\tSteps: 7668304\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.216614\tSteps: 7746704\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.217924\tSteps: 7825104\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.198440\tSteps: 7903504\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.206105\tSteps: 7981904\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.189964\tSteps: 8060304\n",
      "\n",
      "Test set: Average loss: 0.2846, Accuracy: 8999/10000 (89%)\n",
      "\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.207422\tSteps: 8168496\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.200846\tSteps: 8246896\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.219167\tSteps: 8325296\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.200916\tSteps: 8403696\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.205691\tSteps: 8482096\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.209593\tSteps: 8560496\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.191283\tSteps: 8638896\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.200550\tSteps: 8717296\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.188134\tSteps: 8795696\n",
      "\n",
      "Test set: Average loss: 0.2826, Accuracy: 9008/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.201112\tSteps: 8903888\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.194781\tSteps: 8982288\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.212650\tSteps: 9060688\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.195373\tSteps: 9139088\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.200676\tSteps: 9217488\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.204657\tSteps: 9295888\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.187105\tSteps: 9374288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.197148\tSteps: 9452688\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.186336\tSteps: 9531088\n",
      "\n",
      "Test set: Average loss: 0.2823, Accuracy: 9023/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.196576\tSteps: 9639280\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.190447\tSteps: 9717680\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.208149\tSteps: 9796080\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.191463\tSteps: 9874480\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.196568\tSteps: 9952880\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.200821\tSteps: 10031280\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.184032\tSteps: 10109680\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.194020\tSteps: 10188080\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.184548\tSteps: 10266480\n",
      "\n",
      "Test set: Average loss: 0.2820, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.192503\tSteps: 10374672\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.187020\tSteps: 10453072\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.204401\tSteps: 10531472\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.188128\tSteps: 10609872\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.192910\tSteps: 10688272\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.197297\tSteps: 10766672\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.181002\tSteps: 10845072\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.191267\tSteps: 10923472\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.182385\tSteps: 11001872\n",
      "\n",
      "Test set: Average loss: 0.2825, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.188985\tSteps: 11110064\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.184014\tSteps: 11188464\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.200923\tSteps: 11266864\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.185075\tSteps: 11345264\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.190258\tSteps: 11423664\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.193950\tSteps: 11502064\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.178032\tSteps: 11580464\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.188968\tSteps: 11658864\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.180160\tSteps: 11737264\n",
      "\n",
      "Test set: Average loss: 0.2827, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.185731\tSteps: 11845456\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.181015\tSteps: 11923856\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.197778\tSteps: 12002256\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.182464\tSteps: 12080656\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.187854\tSteps: 12159056\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.191153\tSteps: 12237456\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.175546\tSteps: 12315856\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.186521\tSteps: 12394256\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.177890\tSteps: 12472656\n",
      "\n",
      "Test set: Average loss: 0.2828, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.182875\tSteps: 12580848\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.178219\tSteps: 12659248\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.194544\tSteps: 12737648\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.179884\tSteps: 12816048\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.185557\tSteps: 12894448\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.188532\tSteps: 12972848\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.172978\tSteps: 13051248\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.184467\tSteps: 13129648\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.175761\tSteps: 13208048\n",
      "\n",
      "Test set: Average loss: 0.2834, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.180076\tSteps: 13316240\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.175838\tSteps: 13394640\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.191933\tSteps: 13473040\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.177498\tSteps: 13551440\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.183273\tSteps: 13629840\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.186200\tSteps: 13708240\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.170682\tSteps: 13786640\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.182308\tSteps: 13865040\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.173822\tSteps: 13943440\n",
      "\n",
      "Test set: Average loss: 0.2837, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.177294\tSteps: 14051632\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.173573\tSteps: 14130032\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.189423\tSteps: 14208432\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.175304\tSteps: 14286832\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.181379\tSteps: 14365232\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.183746\tSteps: 14443632\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.168652\tSteps: 14522032\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.180352\tSteps: 14600432\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.171953\tSteps: 14678832\n",
      "\n",
      "Test set: Average loss: 0.2844, Accuracy: 9045/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set default values for model\n",
    "nummod = \"1\"\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "dropout = 0.0\n",
    "clip = -1\n",
    "epochs = 20\n",
    "kernel_size = 6\n",
    "levels = 8\n",
    "log_interval = 100\n",
    "lr = 2e-3\n",
    "optimm = 'Adam'\n",
    "nhid = 20\n",
    "seed = 1111\n",
    "torch.manual_seed(seed)\n",
    "permutee = False\n",
    "root = '../data/fashion_mnist'\n",
    "input_channels = 1\n",
    "n_classes = 10\n",
    "seq_length = int(784 / input_channels)\n",
    "steps = 0\n",
    "file_name = './best_models/manual_{0}_20e.pt'.format(nummod)\n",
    "classes = ('tshit/top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle/boot')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TCN(input_channels, n_classes, hidden_units=nhid, levels=levels, \n",
    "            kernel_size=kernel_size, dropout=dropout)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "optimizer = getattr(optim, 'Adam')(model.parameters(), lr=lr)\n",
    "steps = 0\n",
    "\n",
    "accuracies_ = []\n",
    "train_losses_ = [[] for i in range(0, epochs)]\n",
    "# test_losses_ = []\n",
    "scores = []\n",
    "train_loader, test_loader = data_generator(root, batch_size)\n",
    "\n",
    "print(\"Model parameters size => {}\".format(count_parameters(model)))\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "        trainTCN(epoch)\n",
    "        testTCN()\n",
    "        if epoch % 10 == 0:\n",
    "#             print('Saving model.....')\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.module.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'train_losses': train_losses_,\n",
    "#                 'test_losses': test_losses_,\n",
    "#                 'accuracies': accuracies_,\n",
    "#                 'curr_lr': lr,\n",
    "#             }, file_name)\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "torch.save(model.module.state_dict(), file_name)\n",
    "scores.append({\n",
    "    'accuracies': accuracies_,\n",
    "    'train_losses': train_losses_,\n",
    "#     'test_losses': test_losses_,\n",
    "})\n",
    "save_obj(scores, './best_models/manual_scores_{}_20e'.format(nummod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2714, Accuracy: 9114/10000 (91%)\n",
      "\n",
      "[[865.   0.  13.  26.   1.   2.  87.   0.   6.   0.]\n",
      " [  2. 986.   0.   6.   1.   0.   3.   0.   2.   0.]\n",
      " [ 15.   1. 880.  12.  49.   0.  42.   0.   1.   0.]\n",
      " [  4.   4.  13. 935.  21.   0.  21.   0.   2.   0.]\n",
      " [  1.   2.  54.  33. 869.   0.  40.   0.   1.   0.]\n",
      " [  0.   0.   0.   1.   0. 970.   0.  14.   0.  15.]\n",
      " [122.   2.  64.  35.  64.   0. 705.   0.   8.   0.]\n",
      " [  0.   0.   0.   0.   0.  15.   0. 948.   0.  37.]\n",
      " [  5.   1.   4.   1.   3.   1.   3.   2. 979.   1.]\n",
      " [  1.   1.   0.   0.   0.   5.   0.  16.   0. 977.]]\n",
      "Accuracy of tshit/top : 86 %\n",
      "Accuracy of trouser : 98 %\n",
      "Accuracy of pullover : 88 %\n",
      "Accuracy of dress : 93 %\n",
      "Accuracy of  coat : 86 %\n",
      "Accuracy of sandal : 97 %\n",
      "Accuracy of shirt : 70 %\n",
      "Accuracy of sneaker : 94 %\n",
      "Accuracy of   bag : 97 %\n",
      "Accuracy of ankle/boot : 97 %\n"
     ]
    }
   ],
   "source": [
    "bst_model_filename = './best_models/paper_tunning_second_7.pt'\n",
    "model = TCN(hidden_units=29, levels=10, kernel_size=6, dropout=0.0)\n",
    "# {'batch_size': 64, 'dropout': 0.0, 'clip': -1.0, '\n",
    "#  lr': 0.0009686968543691799, 'ksize': 6, 'levels': 10, \n",
    "#  'optim': 'Adam', 'nhid': 29, 'epochs': 20}\n",
    "# TCN(input_channels, n_classes, hidden_units=nhid, levels=levels, \n",
    "#             kernel_size=kernel_size, dropout=dropout)\n",
    "# best_model.torch.load(bst_model_filename, map_location=\"cuda:0\")\n",
    "model.load_state_dict(torch.load(bst_model_filename))  \n",
    "model.to(device)\n",
    "_, best_conf = testTCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2844, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "[[858.   0.  15.  25.   2.   2.  95.   0.   3.   0.]\n",
      " [  0. 986.   0.   7.   1.   0.   6.   0.   0.   0.]\n",
      " [ 19.   3. 841.   9.  63.   0.  64.   0.   1.   0.]\n",
      " [ 12.   2.   9. 927.  28.   1.  20.   0.   1.   0.]\n",
      " [  1.   0.  62.  28. 853.   0.  55.   0.   1.   0.]\n",
      " [  0.   0.   0.   0.   0. 964.   0.  18.   0.  18.]\n",
      " [113.   0.  58.  29.  71.   1. 721.   0.   7.   0.]\n",
      " [  0.   0.   0.   0.   0.  15.   0. 952.   1.  32.]\n",
      " [  3.   2.   4.   6.   4.   2.   7.   2. 969.   1.]\n",
      " [  0.   0.   0.   0.   0.   4.   1.  21.   0. 974.]]\n",
      "Accuracy of tshit/top : 85 %\n",
      "Accuracy of trouser : 98 %\n",
      "Accuracy of pullover : 84 %\n",
      "Accuracy of dress : 92 %\n",
      "Accuracy of  coat : 85 %\n",
      "Accuracy of sandal : 96 %\n",
      "Accuracy of shirt : 72 %\n",
      "Accuracy of sneaker : 95 %\n",
      "Accuracy of   bag : 96 %\n",
      "Accuracy of ankle/boot : 97 %\n"
     ]
    }
   ],
   "source": [
    "# # batch_size = 64\n",
    "# # cuda = True\n",
    "# # dropout = 0.0\n",
    "# # clip = -1\n",
    "# # epochs = 20\n",
    "# # kernel_size = 6\n",
    "# # levels = 8\n",
    "# # log_interval = 100\n",
    "# # lr = 2e-3\n",
    "# # optimm = 'Adam'\n",
    "# # nhid = 20\n",
    "# # seed = 1111\n",
    "# # torch.manual_seed(seed)\n",
    "# # permutee = False\n",
    "# # root = '../data/fashion_mnist'\n",
    "# # input_channels = 1\n",
    "# # n_classes = 10\n",
    "# # seq_length = int(784 / input_channels)\n",
    "# # steps = 0\n",
    "# my_best_model = './best_models/manual_1_20e.pt'\n",
    "# model = TCN(hidden_units=20, levels=8, kernel_size=6, dropout=0.0)\n",
    "# # {'batch_size': 64, 'dropout': 0.0, 'clip': -1.0, '\n",
    "# #  lr': 0.0009686968543691799, 'ksize': 6, 'levels': 10, \n",
    "# #  'optim': 'Adam', 'nhid': 29, 'epochs': 20}\n",
    "# # TCN(input_channels, n_classes, hidden_units=nhid, levels=levels, \n",
    "# #             kernel_size=kernel_size, dropout=dropout)\n",
    "# # best_model.torch.load(bst_model_filename, map_location=\"cuda:0\")\n",
    "# model.load_state_dict(torch.load(my_best_model))  \n",
    "# model.to(device)\n",
    "# _, manual_conf = testTCN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.,  0.,  2.,  1.,  1.,  0.,  8.,  0.,  3.,  0.],\n",
       "       [ 2.,  0.,  0.,  1.,  0.,  0.,  3.,  0.,  2.,  0.],\n",
       "       [ 4.,  2., 39.,  3., 14.,  0., 22.,  0.,  0.,  0.],\n",
       "       [ 8.,  2.,  4.,  8.,  7.,  1.,  1.,  0.,  1.,  0.],\n",
       "       [ 0.,  2.,  8.,  5., 16.,  0., 15.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  6.,  0.,  4.,  0.,  3.],\n",
       "       [ 9.,  2.,  6.,  6.,  7.,  1., 16.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  1.,  5.],\n",
       "       [ 2.,  1.,  0.,  5.,  1.,  1.,  4.,  0., 10.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.,  1.,  1.,  5.,  0.,  3.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_conf = np.array(manual_conf)\n",
    "abs(best_conf - manual_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(1, 29, kernel_size=(6,), stride=(1,), padding=(5,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(10,), dilation=(2,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(20,), dilation=(4,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(40,), dilation=(8,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(80,), dilation=(16,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(160,), dilation=(32,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(320,), dilation=(64,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(640,), dilation=(128,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(1280,), dilation=(256,))\n",
      "Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(2560,), dilation=(512,))\n"
     ]
    }
   ],
   "source": [
    "# {'batch_size': 64, 'dropout': 0.0, 'clip': -1.0, 'lr': 0.0009686968543691799, \n",
    "# 'ksize': 6, 'levels': 10, 'optim': 'Adam', 'nhid': 29, 'epochs': 20}\n",
    "len(model.tcn.network)\n",
    "# for temporal_block in model.tcn.network:\n",
    "#     print(temporal_block)\n",
    "fst_layer = model.tcn.network[-1].conv2.weight.data.cpu()\n",
    "fst_layer = fst_layer.numpy()\n",
    "# num_kernels = fst_layer.shape[0]\n",
    "\n",
    "# onv1 = nn.Conv2d(3, 1, 3)\n",
    "# weight = conv1.weight.data.numpy()\n",
    "# plt.imshow(fst_layer[0, ...])\n",
    "# layers = model.tcn.network.children()\n",
    "# for l in layers:\n",
    "#     print(l.net)\n",
    "    \n",
    "layers = [l.net for l in model.tcn.network.children()]\n",
    "for l in layers:\n",
    "    print(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernels(tensor, num_cols=6):\n",
    "    num_kernels = tensor.shape[0]\n",
    "#     print(num_kernels)\n",
    "    print(tensor.shape)\n",
    "    num_rows = 1+ num_kernels // num_cols\n",
    "#     print(num_rows)\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    len(tensor)\n",
    "    for i in range(num_kernels):\n",
    "#         print()\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "#         ax1.imshow(tensor[i][0,:], cmap='gray')\n",
    "#         ax1.axis('off')\n",
    "#         ax1.set_xticklabels([])\n",
    "#         ax1.set_yticklabels([])\n",
    "#     plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Conv1d(1, 29, kernel_size=(6,), stride=(1,), padding=(5,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(5,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(10,), dilation=(2,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(10,), dilation=(2,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(20,), dilation=(4,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(20,), dilation=(4,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(40,), dilation=(8,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(40,), dilation=(8,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(80,), dilation=(16,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(80,), dilation=(16,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(160,), dilation=(32,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(160,), dilation=(32,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(320,), dilation=(64,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(320,), dilation=(64,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(640,), dilation=(128,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(640,), dilation=(128,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(1280,), dilation=(256,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(1280,), dilation=(256,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " ), Sequential(\n",
       "   (0): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(2560,), dilation=(512,))\n",
       "   (1): Chomp1d()\n",
       "   (2): ReLU()\n",
       "   (3): Dropout(p=0.0)\n",
       "   (4): Conv1d(29, 29, kernel_size=(6,), stride=(1,), padding=(2560,), dilation=(512,))\n",
       "   (5): Chomp1d()\n",
       "   (6): ReLU()\n",
       "   (7): Dropout(p=0.0)\n",
       " )]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filters = model.modules();\n",
    "model.tcn.network.children()\n",
    "model_layers = [i.net for i in model.tcn.network.children()];\n",
    "model_layers\n",
    "# plt.imshow(model_layers[0])\n",
    "model_layers[0][0].weight.data.cpu()\n",
    "model_layers\n",
    "# len(model_layers)\n",
    "# first_layer = model_layers[0];\n",
    "# first_layer\n",
    "# second_layer\n",
    "# second_layer = model_layers[1];\n",
    "# filtersl\n",
    "# layers = [l for l in model_layers]\n",
    "# layers\n",
    "# kernels = [l[0].weight.data.cpu() for l in model_layers]\n",
    "# kernels[0].numpy()[0][0]\n",
    "# plot_kernels(kernels[0][0].numpy())\n",
    "# kernels[0][0].numpy()[0][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-bef222f89f55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sise'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
