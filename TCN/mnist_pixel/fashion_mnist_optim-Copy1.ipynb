{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/home/A00512318/TCN')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from TCN.mnist_pixel.model import TCN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "def data_generator(root, batch_size):\n",
    "    train_set = datasets.FashionMNIST(root=root, train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "    test_set = datasets.FashionMNIST(root=root, train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTCN(ep):\n",
    "    global steps\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda: \n",
    "            data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, input_channels, seq_length)\n",
    "        if permutee:\n",
    "            data = data[:, :, permute]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        if parameters['clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), parameters['clip'])\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_losses_.append(train_loss)\n",
    "        steps += seq_length\n",
    "        if batch_idx > 0 and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tSteps: {}'.format(\n",
    "                ep, batch_idx * parameters['batch_size'], len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), train_loss/log_interval, steps))\n",
    "            train_loss = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTCN():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    correct_class = list(0. for i in range(10))\n",
    "    correct_total = list(0. for i in range(10))\n",
    "    tot = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, input_channels, seq_length)\n",
    "            if permutee:\n",
    "                data = data[:, :, permute]\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            _, pred = torch.max(output, 1)\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "#             print(len(target.data.view_as(pred)))\n",
    "            c = (pred == target).squeeze()\n",
    "            tot += 1\n",
    "#             if tot != 313:\n",
    "#             for i in range(len(test_loader.dataset) // batch_size):\n",
    "# #                     print(pred[i], target.data.view_as(pred)[i])\n",
    "#                 print(i)\n",
    "#                 label = pred[i]\n",
    "#                 if (pred[i] == target.data.view_as(pred)[i]):\n",
    "#                     correct_class[label] += c[i].item()\n",
    "#                 correct_total[label] += 1\n",
    "                    \n",
    "                \n",
    "#     print(tot)\n",
    "#     for i in range(10):\n",
    "#         print('Accuracy of %5s : %2d %%' % (\n",
    "#             classes[i], 100 * correct_class[i] / correct_total[i]))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "#     print(correct.item())\n",
    "    accuracies_.append(correct.item() / 10000.)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Print model's state_dict\n",
    "def models_state_dict(model):\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "def optimizers_state_dict():\n",
    "    print(\"Optimizer's state_dict:\")\n",
    "    for var_name in optimizer.state_dict():\n",
    "        print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_list_params(obj):\n",
    "    with open('list_params.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_list_params():\n",
    "    with open('list_params.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batch_size': 32,\n",
       "  'dropout': 0.1934688879645705,\n",
       "  'clip': 0.5143058097996211,\n",
       "  'lr': 1.8394982956044217e-05,\n",
       "  'ksize': 5,\n",
       "  'levels': 6,\n",
       "  'optim': 'Adam',\n",
       "  'nhid': 21,\n",
       "  'epochs': 5},\n",
       " {'batch_size': 128,\n",
       "  'dropout': 0.3203992070215431,\n",
       "  'clip': 0.8603532372826708,\n",
       "  'lr': 0.00932323004931024,\n",
       "  'ksize': 5,\n",
       "  'levels': 5,\n",
       "  'optim': 'Adam',\n",
       "  'nhid': 27,\n",
       "  'epochs': 1}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "num_models = 50\n",
    "seen_hyper = set()\n",
    "list_parameters = [{} for i in range(0, num_models)]\n",
    "for i in range(0, num_models):\n",
    "    while(True):\n",
    "        list_parameters[i]['batch_size'] = random.choice([16, 32, 64, 128])\n",
    "        list_parameters[i]['dropout'] = np.random.uniform(0., 1)\n",
    "        list_parameters[i]['clip'] = np.random.uniform(0.3, 1)\n",
    "        list_parameters[i]['lr'] = pow(10, np.random.uniform(-6, 1))\n",
    "        list_parameters[i]['ksize'] = np.random.randint(2, 10)\n",
    "        list_parameters[i]['levels'] = np.random.randint(2, 11)\n",
    "        list_parameters[i]['optim'] = 'Adam'\n",
    "        list_parameters[i]['nhid'] = np.random.randint(15, 30)\n",
    "        list_parameters[i]['epochs'] = np.random.randint(1, 6)\n",
    "        k = \"\".join(str(v) + '_' for k, v in list_parameters[i].items())\n",
    "        if k not in seen_hyper:\n",
    "            seen_hyper.add(k)\n",
    "            break\n",
    "save_list_params(list_parameters)\n",
    "[param for param in list_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'dropout': 0.1934688879645705, 'clip': 0.5143058097996211, 'lr': 1.8394982956044217e-05, 'ksize': 5, 'levels': 6, 'optim': 'Adam', 'nhid': 21, 'epochs': 5}\n",
      "25126\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.428643\tSteps: 79184\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.382648\tSteps: 157584\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.371411\tSteps: 235984\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.350071\tSteps: 314384\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.347870\tSteps: 392784\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.334228\tSteps: 471184\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.323529\tSteps: 549584\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.312629\tSteps: 627984\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.311389\tSteps: 706384\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.297698\tSteps: 784784\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.290565\tSteps: 863184\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.285185\tSteps: 941584\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.264798\tSteps: 1019984\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.249186\tSteps: 1098384\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.244303\tSteps: 1176784\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.220043\tSteps: 1255184\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.200242\tSteps: 1333584\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.174475\tSteps: 1411984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/A00512318/anaconda3/envs/tcn/lib/python3.7/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1628, Accuracy: 1948/10000 (19%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.176939\tSteps: 1549184\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.134580\tSteps: 1627584\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.117273\tSteps: 1705984\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.091464\tSteps: 1784384\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 2.062406\tSteps: 1862784\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.052197\tSteps: 1941184\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 2.025826\tSteps: 2019584\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.999580\tSteps: 2097984\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 1.982327\tSteps: 2176384\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.955436\tSteps: 2254784\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 1.946864\tSteps: 2333184\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.922619\tSteps: 2411584\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 1.907333\tSteps: 2489984\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.880842\tSteps: 2568384\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.878207\tSteps: 2646784\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.827849\tSteps: 2725184\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 1.825237\tSteps: 2803584\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.814507\tSteps: 2881984\n",
      "\n",
      "Test set: Average loss: 1.7898, Accuracy: 3729/10000 (37%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 1.799323\tSteps: 3019184\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.781099\tSteps: 3097584\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 1.765472\tSteps: 3175984\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.777551\tSteps: 3254384\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 1.728299\tSteps: 3332784\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.729675\tSteps: 3411184\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 1.700116\tSteps: 3489584\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.675774\tSteps: 3567984\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 1.677267\tSteps: 3646384\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.656557\tSteps: 3724784\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 1.640672\tSteps: 3803184\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.631193\tSteps: 3881584\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 1.622468\tSteps: 3959984\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.620415\tSteps: 4038384\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.613949\tSteps: 4116784\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.575209\tSteps: 4195184\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 1.569938\tSteps: 4273584\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.576985\tSteps: 4351984\n",
      "\n",
      "Test set: Average loss: 1.5533, Accuracy: 4180/10000 (41%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 1.563632\tSteps: 4489184\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.545968\tSteps: 4567584\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 1.550990\tSteps: 4645984\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.547719\tSteps: 4724384\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 1.520948\tSteps: 4802784\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.523786\tSteps: 4881184\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 1.501272\tSteps: 4959584\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.495562\tSteps: 5037984\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 1.500916\tSteps: 5116384\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.473804\tSteps: 5194784\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 1.461549\tSteps: 5273184\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.457375\tSteps: 5351584\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 1.440442\tSteps: 5429984\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.454544\tSteps: 5508384\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.463169\tSteps: 5586784\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.438412\tSteps: 5665184\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 1.411377\tSteps: 5743584\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.425962\tSteps: 5821984\n",
      "\n",
      "Test set: Average loss: 1.4205, Accuracy: 4499/10000 (44%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 1.427234\tSteps: 5959184\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.422857\tSteps: 6037584\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 1.413295\tSteps: 6115984\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.395136\tSteps: 6194384\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 1.387211\tSteps: 6272784\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.389046\tSteps: 6351184\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 1.380321\tSteps: 6429584\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.366496\tSteps: 6507984\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 1.381296\tSteps: 6586384\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.378932\tSteps: 6664784\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 1.358427\tSteps: 6743184\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.354219\tSteps: 6821584\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 1.343302\tSteps: 6899984\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.343013\tSteps: 6978384\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 1.350504\tSteps: 7056784\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.340089\tSteps: 7135184\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 1.304658\tSteps: 7213584\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.327940\tSteps: 7291984\n",
      "\n",
      "Test set: Average loss: 1.3175, Accuracy: 4889/10000 (48%)\n",
      "\n",
      "Saving checkpoint for model.....\n",
      "Saved as model_0.pt\n",
      "{'batch_size': 128, 'dropout': 0.3203992070215431, 'clip': 0.8603532372826708, 'lr': 0.00932323004931024, 'ksize': 5, 'levels': 5, 'optim': 'Adam', 'nhid': 27, 'epochs': 1}\n",
      "33814\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-06785932fbdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtrainTCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mtestTCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-60f3f4b5ea83>\u001b[0m in \u001b[0;36mtrainTCN\u001b[0;34m(ep)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tcn/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_channels = 1 # just one for the image\n",
    "classes = ('T-shirt/Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n",
    "n_classes = len(classes) # 10 classes for fashion-mnist\n",
    "log_interval = 100\n",
    "seed = 1111\n",
    "torch.manual_seed(seed)\n",
    "permutee = False\n",
    "input_channels = 1\n",
    "seq_length = int(784 / input_channels)\n",
    "\n",
    "cuda = True\n",
    "steps = 0\n",
    "for model_indx, parameters in enumerate(list_parameters):\n",
    "    print(parameters)\n",
    "    steps = 0\n",
    "#     permute = torch.Tensor(np.random.permutation(784).astype(np.float64)).long()\n",
    "#     permute = permute.to(device)\n",
    "    \n",
    "    train_loader, test_loader = data_generator('../data/fashion_mnist', parameters['batch_size'])\n",
    "    \n",
    "    model = TCN(input_channels, n_classes, hidden_units=parameters['nhid'], levels=parameters['levels'], kernel_size=parameters['ksize'], dropout=parameters['dropout'])\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    print(count_parameters(model))\n",
    "    lr = parameters['lr']\n",
    "    optimizer = getattr(optim, parameters['optim'])(model.parameters(), lr=lr)\n",
    "    accuracies_ = []\n",
    "    train_losses_ = []\n",
    "    test_losses_ = []\n",
    "    file_name = 'model_{0}.pt'.format(model_indx)\n",
    "   \n",
    "    for epoch in range(1, parameters['epochs']+1):\n",
    "        trainTCN(epoch)\n",
    "        testTCN()\n",
    "        if epoch % 5 == 0: \n",
    "            print('Saving checkpoint for model.....')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.module.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses_,\n",
    "                'test_losses': test_losses_,\n",
    "                'accuracies': accuracies_,\n",
    "                'curr_lr': lr,\n",
    "            }, file_name)\n",
    "        if epoch % 10 == 0:\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "    torch.save({\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'accuracies': accuracies_,\n",
    "        'train_losses': train_losses_,\n",
    "        'test_losses': test_losses_,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, file_name)\n",
    "    print('Saved as %s' % file_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TCN' object has no attribute 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1e20683fb4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_0.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tcn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TCN' object has no attribute 'module'"
     ]
    }
   ],
   "source": [
    "mmodel = TCN()\n",
    "mmodel.load_state_dict(torch.load('model_0.pt')['model_state_dict'])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
